{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding Using IMDB Dataset\n",
        "1. What is the IMDB Dataset?\n",
        "\n",
        "A text dataset of 50,000 movie reviews\n",
        "\n",
        "Reviews are labeled as positive or negative\n",
        "\n",
        "Available directly in Keras\n",
        "\n",
        "2. What Is Word Embedding?\n",
        "\n",
        "A method to convert words into dense numerical vectors\n",
        "\n",
        "Each word is represented by a learned vector (e.g., 100-dimensional)\n",
        "\n",
        "Unlike one-hot encoding, embedding vectors capture:\n",
        "\n",
        "Meaning\n",
        "\n",
        "Context\n",
        "\n",
        "Similarity between words\n",
        "\n",
        " 3. Why Use Word Embeddings?\n",
        "\n",
        "Reduces dimensionality\n",
        "\n",
        "Captures semantic relationships\n",
        "\n",
        "Helps neural networks understand text\n",
        "\n",
        "Performs better than one-hot encoding\n",
        "\n",
        "4. Workflow: IMDB + Word Embedding\n",
        "\n",
        "Load the IMDB dataset\n",
        "\n",
        "Pad sequences (so all reviews have equal length)\n",
        "\n",
        "Use an Embedding layer to convert word indices → dense vectors\n",
        "\n",
        "Feed these vectors into a neural network (RNN / LSTM / CNN / Dense)\n",
        "\n",
        "Train the model for sentiment classification"
      ],
      "metadata": {
        "id": "uWhNzQ8YCz-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load dataset\n",
        "# ---------------------------\n",
        "num_words = 10000   # Only keep top 10,000 words\n",
        "maxlen = 200        # Cut/pad reviews to 200 words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Pad sequences so all are of same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Build model with Embedding\n",
        "# ---------------------------\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),  # Word embeddings\n",
        "    LSTM(64),                # Recurrent layer to capture sequence info\n",
        "    Dense(1, activation='sigmoid')   # Binary classification (positive/negative)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train model\n",
        "# ---------------------------\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.2, verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Evaluate model\n",
        "# ---------------------------\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_NNvtdDCch",
        "outputId": "961cf3dd-90e9-4c0a-d207-d2522437ecab"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 377ms/step - accuracy: 0.6861 - loss: 0.5669 - val_accuracy: 0.8606 - val_loss: 0.3418\n",
            "Epoch 2/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 373ms/step - accuracy: 0.8915 - loss: 0.2748 - val_accuracy: 0.8760 - val_loss: 0.3056\n",
            "Epoch 3/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 404ms/step - accuracy: 0.9374 - loss: 0.1782 - val_accuracy: 0.8626 - val_loss: 0.3810\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - accuracy: 0.8518 - loss: 0.4065\n",
            "Test Accuracy: 0.8504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrJfvCfZEaQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}